from __future__ import annotations

# === Export Helpers: export plan to ICS (calendar) ===
def export_plan_ics(schedule: dict[int, list[str]], terms: list[str], df_all: pd.DataFrame,
                    out_path: str = "plan.ics") -> str:
    import datetime as _dt
    def _term_start_date(term: str, index1: int) -> _dt.date:
        t = (term or "").upper()
        if "T1" in t:  month, day = 3, 1
        elif "T2" in t: month, day = 6, 1
        elif "T3" in t: month, day = 9, 1
        elif "SUMMER" in t or "T0" in t: month, day = 12, 1
        else: month, day = 1, 1
        y = _dt.date.today().year + (0 if index1 <= 3 else 1)
        return _dt.date(y, month, day)
    lines = ["BEGIN:VCALENDAR", "VERSION:2.0", "PRODID:-//UNSW Course Agent//EN"]
    for i, term in enumerate(terms, 1):
        start = _term_start_date(term, i)
        for code in (schedule.get(i, []) or []):
            hit = df_all[df_all["CourseCode"].str.upper()==str(code).upper()]
            name = hit.iloc[0].get("CourseName", "") if not hit.empty else ""
            desc = hit.iloc[0].get("Description", "") if not hit.empty else ""
            desc = (desc or "").replace("\r", " ").replace("\n", " ")
            uid = f"{code}-{term}-{i}@unsw-agent"
            dtstart = start.strftime("%Y%m%d")
            dtend = (start + _dt.timedelta(days=1)).strftime("%Y%m%d")
            lines += [
                "BEGIN:VEVENT",
                f"UID:{uid}",
                f"SUMMARY:{code} â€“ {name} [{term}]",
                f"DTSTART;VALUE=DATE:{dtstart}",
                f"DTEND;VALUE=DATE:{dtend}",
                f"DESCRIPTION:{desc[:800]}",
                "END:VEVENT",
            ]
    lines += ["END:VCALENDAR"]
    content = "\r\n".join(lines) + "\r\n"
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(content)
    return out_path
# -*- coding: utf-8 -*-
"""
ğŸ“ UNSW Course Agent
- å¤šè¯¾ç¨‹ä¸¥æ ¼åŒ¹é…ï¼ˆDataFrameï¼‰+ å°¾å·/æ¨¡ç³ŠåŒ¹é… + å‘é‡æ£€ç´¢å…œåº•
- å¯¹è¯å›æŒ‡ï¼šæ”¯æŒâ€œå®ƒ/è¿™ä¸ª/å‰ç½®/äº’æ–¥/æè¿°â€ç­‰ï¼›Web ç«¯æŠŠä¸Šä¸‹æ–‡æ‹¼æ¥ç»™åç«¯
- è¯¾ç¨‹ä¿¡æ¯æŸ¥è¯¢ï¼šå¼€è¯¾å­¦æœŸ/å…ˆä¿®/äº’æ–¥/ç­‰ä»·/ç±»åˆ«/ç®€ä»‹ å…¨è¦†ç›–
- AI æ–¹å‘ä¸¤å¹´é€‰è¯¾å»ºè®®ï¼ˆç²¾å‡†æ¯•ä¸šè§„åˆ™ï¼‰
  â€¢ æ€»è®¡ 96 UOCï¼›Capstone/Research 18 UOCï¼›é Capstone 78 UOCï¼ˆ13Ã—6UOCï¼‰
  â€¢ ä¸¥æ ¼ç±»åˆ«æ˜ å°„ + Elective ç™½åå• + DKE åˆ—è¡¨
  â€¢ è·¯çº¿åå¥½ï¼šProjectï¼ˆCOMP9900 + GSOE9010/9011 + â‰¥1 DKEï¼‰
             æˆ– Researchï¼ˆCOMP9991+9993ï¼›æˆ– 9991+9992 + â‰¥1 DKEï¼‰
  â€¢ æ”¯æŒâ€œå·²ä¿®è¯¾ç¨‹â€ï¼ˆå¦‚ COMP9021/9024/â€¦ï¼‰ã€â€œæ’é™¤è¯¾ç¨‹/ä¸»é¢˜â€ï¼ˆå¦‚ ä¸è¦9414/ä¸è¦CVï¼‰ã€
    â€œå­¦æœŸè´Ÿè½½â€ï¼ˆ233 233 / 332 332 ç­‰ï¼‰
  â€¢ æ’è¯¾ç­–ç•¥ï¼šå…ˆ Foundational Coreï¼Œå†äº¤é”™ Adv/AI/DKE/Electiveï¼›éµå®ˆå¼€è¯¾å­¦æœŸä¸å…ˆä¿®
  â€¢ è§£é‡ŠæŒ‰éœ€ï¼šå…ˆç”Ÿæˆè®¡åˆ’ï¼Œå†è¯´â€œè¯·ç»™è§£é‡Šâ€â†’ é€é—¨è§£é‡Š + å¼•ç”¨ï¼ˆæ¥è‡ªæœ¬åœ° CSVï¼‰
- è¯¾ç¨‹å£ç¢‘ï¼ˆRAGï¼Œä»…æœ¬åœ° JSONLï¼‰
  â€¢ æ±‡æ€» è¯„åˆ†/éš¾åº¦/å·¥ä½œé‡ + äº®ç‚¹/ç—›ç‚¹ + ä»£è¡¨æ€§è¯„è®º
  â€¢ æ”¯æŒå¯¹æ¯”ï¼šå¦‚â€œ9414 vs 9814 å“ªä¸ªæ›´æ¨èâ€
- å¯¼å‡ºåŠŸèƒ½
  â€¢ â€œå¯¼å‡ºè®¡åˆ’â€ â†’ ç”Ÿæˆ plan.csvï¼ˆTerm, CourseCode, CourseName, Descriptionï¼‰
  â€¢ â€œå¯¼å‡ºæ—¥å†â€ â†’ ç”Ÿæˆ plan.icsï¼ˆæ¯é—¨è¯¾ 1 æ¡ All-day äº‹ä»¶ï¼Œå¯å¯¼å…¥ Google/Apple/Outlookï¼‰
- Web UIï¼ˆGradioï¼‰
  â€¢ ui_gradio.py è°ƒç”¨ agent_respond()ï¼Œå†…ç½®ç¤ºä¾‹é—®é¢˜ä¸ä¸€é”®å¯¼å‡ºæŒ‰é’®

ä¾èµ–ï¼š
  pip install -U langgraph langchain-community dashscope python-dotenv faiss-cpu pandas gradio

å‡†å¤‡ï¼š
  - å°† COMPLS_courses.csv ä¸æœ¬æ–‡ä»¶æ”¾åŒä¸€ç›®å½•ï¼ˆæˆ–åœ¨ä»£ç ä¸­è°ƒæ•´è·¯å¾„ï¼‰
  - åœ¨ .env ä¸­è®¾ç½® DASHSCOPE_API_KEY
  - ï¼ˆå¯é€‰ï¼‰å‡†å¤‡ course_reviews.jsonlï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰

è¿è¡Œï¼š
  # å‘½ä»¤è¡Œ
  python UNSW_Course_Agent.py

  # ç½‘é¡µç‰ˆï¼ˆGradioï¼‰
  python ui_gradio.py   â†’ æ‰“å¼€ http://127.0.0.1:7860/

å¸¸ç”¨ç¤ºä¾‹ï¼š
  - ç»™æˆ‘AIä¸¤å¹´é€‰è¯¾å»ºè®® æˆ‘è¦project 233 233 ä¸è¦9414
  - è¯·ç»™è§£é‡Š
  - 9414åœ¨Tå‡  / å®ƒçš„è¯¾ç¨‹æè¿° / å®ƒçš„å‰ç½®
  - COMP9414 è¯„ä»·æ€ä¹ˆæ · / 9414 vs 9814 å“ªä¸ªæ›´æ¨è
  - å¯¼å‡ºè®¡åˆ’ / å¯¼å‡ºæ—¥å†
"""

import os, re, json
import pandas as pd
from typing import TypedDict, Annotated, List, Any, Optional
from dotenv import load_dotenv

from dashscope import Generation
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain_core.messages import HumanMessage

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

# ---------------- Config ----------------
load_dotenv()
CSV_CANDIDATES = [
    "COMPLS_courses.csv",
]
CSV_FILE = next((p for p in CSV_CANDIDATES if os.path.exists(p)), CSV_CANDIDATES[-1])
VECTOR_DB_PATH = "course_vector_db_v4"
EMBED_MODEL = "text-embedding-v1"
LLM_MODEL = "qwen-turbo"
TOP_K = 6

api_key = os.getenv("DASHSCOPE_API_KEY")
if not api_key:
    raise EnvironmentError("âŒ DASHSCOPE_API_KEY æœªé…ç½®ï¼Œè¯·åœ¨ .env ä¸­è®¾ç½®ã€‚")
#print("ğŸ”‘ DASHSCOPE_API_KEY:", api_key[:8] + "..." + api_key[-4:])

# ---------------- Load Data ----------------
print("ğŸ“˜ åŠ è½½è¯¾ç¨‹æ•°æ®:", CSV_FILE)
df = pd.read_csv(CSV_FILE, dtype=str).fillna("")
df["CourseCode"] = df["CourseCode"].astype(str).str.upper().str.replace(" ", "", regex=False)
df = df[df["CourseCode"].str.match(r"^[A-Z]{4}\d{4}$", na=False)].copy()
if df.empty:
    raise RuntimeError("è¯¾ç¨‹æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ CSVã€‚")

# Build retrieval docs (optional)
vectorstore = None
try:
    docs = []
    for _, r in df.iterrows():
        parts = [
            f"{r['CourseCode']} - {r.get('CourseName','')}",
            f"{r.get('Credits','')}UOC, {r.get('OfferingTerms','')}",
            r.get('Description',''),
        ]
        if r.get('Category'): parts.append(f"Category: {r['Category']}")
        if r.get('ConditionsForEnrolment'): parts.append(f"Prereqs: {r['ConditionsForEnrolment']}")
        if r.get('EquivalentCourses'): parts.append(f"Equivalent: {r['EquivalentCourses']}")
        if r.get('ExclusionCourses'): parts.append(f"Exclusion: {r['ExclusionCourses']}")
        docs.append(Document(page_content="\n".join([x for x in parts if str(x).strip()])))
    embeddings = DashScopeEmbeddings(model=EMBED_MODEL, dashscope_api_key=api_key)
    if not os.path.exists(VECTOR_DB_PATH):
        print("ğŸ§  ç”Ÿæˆå‘é‡æ•°æ®åº“...")
        vectorstore = FAISS.from_documents(docs, embeddings)
        vectorstore.save_local(VECTOR_DB_PATH)
    else:
        print("ğŸ“‚ åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“...")
        vectorstore = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
except Exception as e:
    print("âš ï¸ æ£€ç´¢å‘é‡åº“ä¸å¯ç”¨ï¼š", e)
    vectorstore = None



# === Course Reviews Store (RAG ä»…ç”¨äºâ€œè¯¾ç¨‹è¯„ä»·â€) ===
import json
from collections import Counter, defaultdict

REVIEWS_FILE = "course_reviews.jsonl"
try:
    _reviews_raw = []
    with open(REVIEWS_FILE, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line: 
                continue
            _reviews_raw.append(json.loads(line))
except FileNotFoundError:
    _reviews_raw = []

# æŒ‰è¯¾ç¨‹å·èšåˆ
REVIEWS_BY_CODE: dict[str, list[dict]] = defaultdict(list)
for r in _reviews_raw:
    code = str(r.get("code","")).upper().strip()
    if code:
        REVIEWS_BY_CODE[code].append(r)

# === Reviews é…ç½®ï¼šè¿‘ä¸¤å¹´è¿‡æ»¤ + æ¥æºåŠ æƒ ===
from datetime import date
import re as _re

def _two_digit_year(y: int) -> str:
    return f"{y%100:02d}"

RECENT_YEARS_DEFAULT: set[str] = {_two_digit_year(date.today().year), _two_digit_year(date.today().year - 1)}

SOURCE_WEIGHTS_DEFAULT: dict[str, float] = {
    "å†…éƒ¨é—®å·": 1.0,
    "ç¾¤å†…è°ƒç ”": 0.9,
    "åŒ¿åæ”¶é›†": 0.8,
}

def _term_year_prefix(term: str) -> str:
    m = _re.match(r"(\d{2})", str(term or ""))
    return m.group(1) if m else ""

def _is_recent_term(term: str, recent_years: set[str] | None) -> bool:
    if not recent_years:
        return True
    return _term_year_prefix(term) in recent_years

def _src_weight(src: str, weights: dict[str, float] | None) -> float:
    if not weights:
        return 1.0
    return float(weights.get(str(src or ""), 1.0))

def _fmt_pct(x: float) -> str:
    try:
        return f"{x*100:.0f}%"
    except Exception:
        return "0%"

def summarize_reviews_for(code: str) -> str:
    code = str(code).upper().strip()
    items = REVIEWS_BY_CODE.get(code, [])
    if not items:
        return f"æš‚æ—  {code} çš„è¯„ä»·ã€‚å¯åˆ›å»º {REVIEWS_FILE} å¹¶è¿½åŠ è¯¥è¯¾ç¨‹çš„è¯„ä»·è®°å½•ï¼ˆJSONLï¼‰ã€‚"

    # ç»Ÿè®¡
    ratings = [r.get("rating") for r in items if isinstance(r.get("rating"), (int, float))]
    diffs = [r.get("difficulty") for r in items if isinstance(r.get("difficulty"), (int, float))]
    wls = [str(r.get("workload","")).lower() for r in items if r.get("workload")]
    pros = [p for r in items for p in (r.get("pros") or [])]
    cons = [c for r in items for c in (r.get("cons") or [])]

    import statistics as S
    avg_rating = round(S.mean(ratings), 1) if ratings else None
    avg_diff = round(S.mean(diffs), 1) if diffs else None
    wl_cnt = Counter(wls)
    wl_total = sum(wl_cnt.values()) or 1

    def _brief(r):
        t = (r.get("comment") or "").strip().replace("\n"," ")
        if len(t) > 120: t = t[:120] + "..."
        meta = " / ".join([s for s in [r.get("term"), r.get("author"), r.get("source")] if s])
        return f"â€œ{t}â€ â€”â€” {meta}" if t else ""

    reps = [_brief(r) for r in items[:3]]
    reps = [x for x in reps if x]

    lines = [f"â­ {code} è¯¾ç¨‹å£ç¢‘ï¼ˆ{len(items)} æ¡ï¼‰"]
    if avg_rating is not None: lines.append(f"- ç»¼åˆè¯„åˆ†ï¼š{avg_rating}/5")
    if avg_diff is not None: lines.append(f"- éš¾åº¦ï¼š{avg_diff}/5")
    if wl_cnt:
        lines.append(f"- å·¥ä½œé‡ï¼šheavy {_fmt_pct(wl_cnt.get('heavy',0)/wl_total)} / "
                     f"medium {_fmt_pct(wl_cnt.get('medium',0)/wl_total)} / "
                     f"light {_fmt_pct(wl_cnt.get('light',0)/wl_total)}")
    if pros: 
        from collections import Counter as _C
        top = [w for w,_ in _C(pros).most_common(3)]
        lines.append(f"- äº®ç‚¹ï¼š{'ã€'.join(top)}")
    if cons:
        from collections import Counter as _C
        top = [w for w,_ in _C(cons).most_common(3)]
        lines.append(f"- ç—›ç‚¹ï¼š{'ã€'.join(top)}")
    if reps:
        lines.append("- ä»£è¡¨æ€§è¯„è®ºï¼š")
        for s in reps:
            lines.append("  Â· " + s)

    lines.append(f"ğŸ“ å¼•ç”¨ï¼šæ¥è‡ªæœ¬åœ°è¯„ä»·åº“ {REVIEWS_FILE}ã€‚")
    return "\n".join(lines)

def compare_reviews(a: str, b: str) -> str:
    sa, sb = summarize_reviews_for(a), summarize_reviews_for(b)
    return f"{sa}\n\nâ€”â€”â€” å¯¹æ¯” â€”â€”\n\n{sb}"
# =========================================================
# Last plan cache (for on-demand explain)
# =========================================================
LAST_PLAN_SCHEDULE = None
LAST_PLAN_TERMS = None
LAST_PLAN_STATE = {}

# === Export Helpers: export plan to CSV ===
def export_plan_csv(schedule: dict[int, list[str]], terms: list[str], df_all: pd.DataFrame,
                    out_path: str = "plan.csv", include_desc: bool = True) -> str:
    rows = []
    for i, term in enumerate(terms, 1):
        codes = schedule.get(i, []) or []
        for code in codes:
            hit = df_all[df_all["CourseCode"].str.upper()==str(code).upper()]
            if not hit.empty:
                r = hit.iloc[0]
                rows.append({
                    "Term": term,
                    "CourseCode": str(code).upper(),
                    "CourseName": r.get("CourseName",""),
                    "Description": r.get("Description","") if include_desc else "",
                })
            else:
                rows.append({
                    "Term": term,
                    "CourseCode": str(code).upper(),
                    "CourseName": "",
                    "Description": "",
                })
    try:
        import pandas as _pd
        _pd.DataFrame(rows).to_csv(out_path, index=False, encoding="utf-8-sig")
    except Exception:
        import csv
        with open(out_path, "w", encoding="utf-8-sig", newline="") as f:
            w = csv.DictWriter(f, fieldnames=["Term","CourseCode","CourseName","Description"])
            w.writeheader()
            w.writerows(rows)
    return out_path
# =========================================================
# Utilities
# =========================================================
def extract_text(result):
    try:
        if isinstance(result, dict):
            return result.get("output", {}).get("text", "") or result.get("text", "") or str(result)
        if hasattr(result, "output") and isinstance(result.output, dict):
            return result.output.get("text", "")
        if hasattr(result, "output_text"):
            return getattr(result, "output_text", "")
        return str(result)
    except Exception:
        return ""

def safe_answer(content):
    if content is None:
        return {"answer": [HumanMessage(content="âš ï¸ æ²¡æœ‰å¯è¿”å›çš„å†…å®¹ã€‚")]}
    if isinstance(content, str):
        return {"answer": [HumanMessage(content=content)]}
    if isinstance(content, list):
        msgs = []
        for c in content:
            msgs.append(HumanMessage(content=str(c)) if not isinstance(c, HumanMessage) else c)
        return {"answer": msgs}
    return {"answer": [HumanMessage(content=str(content))]}

def _extract_latest_user_utterance(text: str) -> str:
    if not isinstance(text, str):
        return str(text)
    patterns = [
        r"(?:^|\n)\s*ç”¨æˆ·[:ï¼š](.+?)(?=\n\s*åŠ©æ‰‹[:ï¼š]|$)",
        r"(?:^|\n)\s*User[:ï¼š](.+?)(?=\n\s*(?:Assistant|Bot)[:ï¼š]|$)",
        r"(?:^|\n)\s*You[:ï¼š](.+?)(?=\n\s*(?:Assistant|Bot)[:ï¼š]|$)",
    ]
    matches = []
    for pat in patterns:
        matches += list(re.finditer(pat, text, flags=re.S | re.I))
    if matches:
        return matches[-1].group(1).strip()
    return text.strip()

def _recent_codes_from_context(raw_text: str, limit: int = 1):
    text = str(raw_text or "").upper()
    codes = re.findall(r"(?<![A-Z0-9])([A-Z]{4}\d{4})(?![A-Z0-9])", text)
    if not codes:
        return []
    picked, seen = [], set()
    for c in reversed(codes):
        if c not in seen:
            picked.append(c); seen.add(c)
            if len(picked) >= limit:
                break
    return list(reversed(picked))

# =========================================================
# Parsing & lookups
# =========================================================
COURSE_PREFIX_TRY = ["COMP","ENGG","INFS","MATH","SENG","DATA"]

def _extract_candidates_from_query(q: str):
    qU = str(q or "").upper()
    full = re.findall(r"(?<![A-Z0-9])([A-Z]{4}\d{4})(?![A-Z0-9])", qU)
    nums = re.findall(r"(?<![A-Z0-9])(\d{4})(?![A-Z0-9])", qU)
    cands = list(dict.fromkeys(full))
    for n in nums:
        for pre in COURSE_PREFIX_TRY:
            c = f"{pre}{n}"
            if c not in cands:
                cands.append(c)
    return cands, nums

def lookup_strict(query: str) -> Optional[pd.DataFrame]:
    q = str(query or "")
    cands, nums = _extract_candidates_from_query(q)
    hits = pd.DataFrame()
    for code in cands:
        hit = df[df["CourseCode"].str.upper().eq(code)]
        if not hit.empty:
            hits = pd.concat([hits, hit])
    for n in nums:
        tail_hit = df[df["CourseCode"].str.upper().str.endswith(n)]
        if not tail_hit.empty:
            hits = pd.concat([hits, tail_hit])
    if hits.empty:
        return None
    hits = hits.drop_duplicates(subset="CourseCode").sort_values("CourseCode").reset_index(drop=True)
    return hits

def lookup_relaxed(query: str) -> Optional[pd.Series]:
    q = str(query or "")
    cands, nums = _extract_candidates_from_query(q)
    for code in cands:
        hit = df[df["CourseCode"].str.upper().eq(code)]
        if not hit.empty:
            return hit.iloc[0]
    for n in nums:
        tail_hit = df[df["CourseCode"].str.upper().str.endswith(n)]
        if not tail_hit.empty:
            comp_first = tail_hit[tail_hit["CourseCode"].str.upper().str.startswith("COMP")]
            return (comp_first.iloc[0] if not comp_first.empty else tail_hit.sort_values("CourseCode").iloc[0])
    for code in cands:
        hit = df[df["CourseCode"].str.upper().str.contains(re.escape(code), na=False)]
        if not hit.empty:
            return hit.iloc[0]
    if vectorstore is not None:
        try:
            res = vectorstore.similarity_search(str(query), k=1)
            if res:
                header = res[0].page_content.split("\n", 1)[0]
                m = re.search(r"(?<![A-Z0-9])([A-Z]{4}\d{4})(?![A-Z0-9])", header)
                if m:
                    code2 = m.group(1)
                    hit = df[df["CourseCode"].str.upper().eq(code2)]
                    if not hit.empty:
                        return hit.iloc[0]
        except Exception:
            pass
    return None

# =========================================================
# Degree rules (AI) â€” strict categories + elective whitelist + capstone rules
# =========================================================
# â€”â€” ç›®æ ‡å­¦åˆ†ï¼šéCapstone/Researchæ€»è®¡ 78 UOCï¼ˆ13 Ã— 6UOCï¼‰
NON_CAPSTONE_TARGET_UOC = 78
CAPSTONE_TARGET_UOC = 18

# â€”â€” ä¸ªåˆ«è¯¾ç¨‹çš„å­¦åˆ†è¦†ç›–ï¼ˆè‹¥CSVé‡Œæ— å‡†ç¡®UOCï¼Œè¿™é‡Œå…œåº•ï¼‰
UOC_OVERRIDE = {
    "COMP9991": 6,   # Research A
    "COMP9992": 6,   # Research B
    "COMP9993": 12,  # Research C
}

def _uoc_of(row) -> int:
    code = str(row.get("CourseCode","")).upper()
    if code in UOC_OVERRIDE:
        return int(UOC_OVERRIDE[code])
    try:
        return int(str(row.get("Credits","") or "0").strip())
    except:
        m = re.search(r"(\d+)", str(row.get("Credits","")))
        return int(m.group(1)) if m else 0

# â€”â€” ç²¾ç¡®ç±»åˆ«æ˜ å°„
AI_RULES = {
    "name": "AI",
    "total_uoc": 96,
    "slots_per_term": 3,   # æ¯å­¦æœŸ 3 é—¨
    "years": 2,            # ä¸¤å¹´
    "buckets": [
        {"key": "found_core", "title": "åŸºç¡€æ ¸å¿ƒè¯¾",     "tags": ["foundational core courses"],                 "uoc_min": 18, "uoc_max": 18},
        {"key": "adv_core",   "title": "é«˜çº§æ ¸å¿ƒè¯¾",     "tags": ["advanced core courses"],                    "uoc_min": 18, "uoc_max": 18},
        {"key": "ai_core",    "title": "AIæ ¸å¿ƒè¯¾",       "tags": ["artificial intelligence core courses","ai core"], "uoc_min": 6,  "uoc_max": 6},
        {"key": "dke",        "title": "å­¦ç§‘é€‰ä¿®ï¼ˆDKEï¼‰","tags": ["disciplinary knowledge elective courses","dke"],   "uoc_min": 18, "uoc_max": 18},
        {"key": "elective",   "title": "ä¸€èˆ¬é€‰ä¿®",       "tags": ["electives"],                                "uoc_min": 18, "uoc_max": 18},
        {"key": "project",    "title": "æ¯•ä¸šç ”ç©¶/é¡¹ç›®",  "tags": ["research","capstone","project","research/capstone/project","capston"], "uoc_min": 18, "uoc_max": 18},
    ]
}

# â€”â€” è‡ªåŠ¨åˆ†ç±»ï¼šå½“ CSV Category ä¸ºç©ºæ—¶ï¼Œç”¨è¯¾ç¨‹å·å…œåº•å½’ç±»
FOUND_CORE = {"COMP9020","COMP9021","COMP9024"}
ADV_CORE   = {"COMP9311","COMP9331"}
AI_CORE    = {"COMP9414","COMP9814"}
PROJECT_ROUTE = {"COMP9900","GSOE9010","GSOE9011"}
RESEARCH_ROUTE= {"COMP9991","COMP9992","COMP9993"}
# å¸¸è§ AI æ–¹å‘ DKE
DKE_CODES  = {"COMP4418","COMP9417","COMP9418","COMP9434","COMP9444","COMP9491","COMP9517","COMP9727"}

# Electiveï¼ˆä¸€èˆ¬é€‰ä¿®ï¼‰
ELECTIVE_CODES = {
"COMP4121","COMP4128","COMP4141","COMP4161","COMP4418","COMP4511",
"COMP6080","COMP6131","COMP6441","COMP6443","COMP6445","COMP6447",
"COMP6448","COMP6451","COMP6452","COMP6453","COMP6713","COMP6714",
"COMP6721","COMP6733","COMP6741","COMP6752","COMP6771","COMP6841",
"COMP6843","COMP6845","COMP6991","COMP9032","COMP9044","COMP9101",
"COMP9102","COMP9153","COMP9154","COMP9164","COMP9201","COMP9211",
"COMP9222","COMP9242","COMP9243","COMP9283","COMP9312","COMP9313",
"COMP9315","COMP9319","COMP9321","COMP9332","COMP9333","COMP9334",
"COMP9336","COMP9337","COMP9414","COMP9415","COMP9417","COMP9418",
"COMP9434","COMP9444","COMP9447","COMP9491","COMP9511","COMP9517",
"COMP9727","COMP9814","COMP9920","GSOE9210","GSOE9220",
"MATH5836","MATH5845","MATH5855","MATH5905","MATH5960"
}

def _auto_category_by_code(code: str) -> str:
    u = str(code).upper().strip()
    if u in FOUND_CORE: return "foundational core courses"
    if u in ADV_CORE:   return "advanced core courses"
    if u in AI_CORE:    return "artificial intelligence core courses"
    if u in PROJECT_ROUTE or u in RESEARCH_ROUTE: return "research/capstone/project"
    if u in DKE_CODES:  return "disciplinary knowledge elective courses"
    if u in ELECTIVE_CODES: return "electives"
    return ""   # âš ï¸ ä¸å½’ç±»ï¼Œé¿å…æŠŠæœªçŸ¥è¯¾ç¨‹è¯¯åˆ¤æˆé€‰ä¿®

def _row_auto_cat(row):
    raw = str(row.get("Category","")).strip().lower()
    return raw if raw else _auto_category_by_code(row.get("CourseCode",""))

df["AutoCategory"] = df.apply(_row_auto_cat, axis=1)

# â€”â€” helpers

def _parse_completed_codes(s: str) -> set:
    """
    è§£æç”¨æˆ·å£°æ˜çš„â€œå·²ä¿®è¯¾ç¨‹â€ï¼š
    - æ”¯æŒå®Œæ•´è¯¾å·ï¼šCOMP9021 / MATH5845 / GSOE9011 ...
    - æ”¯æŒè£¸å››ä½æ•°å­—ï¼š9021 -> é»˜è®¤æ˜ å°„ä¸º COMP9021
    - è¿”å›å¤§å†™å»é‡åçš„è¯¾ç¨‹å·é›†åˆ
    """
    text = str(s or "").upper()

    # 1) å…ˆå–å®Œæ•´è¯¾å·ï¼ˆå¦‚ COMP9021 / MATH5845ï¼‰
    full = set(re.findall(r"(?<![A-Z0-9])([A-Z]{4}\d{4})(?![A-Z0-9])", text))

    # 2) å†å–è£¸å››ä½æ•°å­—å¹¶é»˜è®¤æ˜ å°„ä¸º COMP####
    nums = re.findall(r"(?<![A-Z0-9])(\d{4})(?![A-Z0-9])", text)
    for n in nums:
        full.add(f"COMP{n}")

    return full


def _parse_term_loads(s: str) -> list | None:
    """
    ä»ç”¨æˆ·è¾“å…¥é‡Œè§£æ 6 ä¸ªå­¦æœŸçš„è¯¾ç¨‹é—¨æ•°ï¼ˆæ¯å­¦æœŸ 1~3 é—¨ï¼‰ã€‚
    æ”¯æŒç¤ºä¾‹ï¼š
      - "332 332" / "233 233" / "333333"
      - "3-3-2 3-3-2" / "3,3,2,3,3,2"
      - "233233"ï¼ˆç´§å‡‘å†™æ³•ï¼‰
    è¿”å› list[int] é•¿åº¦=6ï¼›ä¸åˆæ³•æ—¶è¿”å› Noneã€‚
    """
    txt = str(s or "").strip()
    if not txt:
        return None

    # æŠŠåˆ†éš”ç¬¦ç»Ÿä¸€æˆç©ºæ ¼
    t = re.sub(r"[^0-9]", " ", txt)
    nums = [n for n in t.split() if n.isdigit()]

    # æƒ…å†µAï¼šç›´æ¥ç»™äº† 6 ä¸ªæ•°å­—
    if len(nums) >= 6:
        loads = [int(x) for x in nums[:6]]
    else:
        # æƒ…å†µBï¼šå¯èƒ½ç»™äº†è¿å†™çš„ 6 ä½ï¼Œæ¯”å¦‚ "233233"
        comp = "".join(nums)
        if len(comp) == 6 and comp.isdigit():
            loads = [int(c) for c in comp]
        else:
            return None

    # æ ¡éªŒèŒƒå›´ 1~3
    if any(x < 1 or x > 3 for x in loads):
        return None
    return loads


# === Exclusion parsing (course codes / topics) ===
NEG_PAT = r"(ä¸è¦|åˆ«|ä¸æƒ³|æ’é™¤|å»æ‰|æ¢æ‰|exclude|drop)"
CODE_PAT = r"(?:COMP|MATH|ELEC|GSOE)\s*-?\s*\d{4}"

TOPIC_SYNS = {
    "ai": {"ai", "äººå·¥æ™ºèƒ½"},
    "ml": {"ml", "æœºå™¨å­¦ä¹ "},
    "cv": {"cv", "è®¡ç®—æœºè§†è§‰", "vision"},
    "nlp": {"nlp", "è‡ªç„¶è¯­è¨€å¤„ç†"},
}

def parse_exclusions(q: str):
    q_low, q_up = (q or "").lower(), (q or "").upper()
    exclude_codes, exclude_topics = set(), set()
    if re.search(NEG_PAT, q_low):
        for c in re.findall(CODE_PAT, q_up):
            exclude_codes.add(re.sub(r"[\s-]+","",c))
        for k, vs in TOPIC_SYNS.items():
            if any(v in q_low for v in vs):
                exclude_topics.add(k)
    return exclude_codes, exclude_topics

def build_search_text(df_):
    return (
        df_["CourseName"].fillna("") + " " +
        df_["Description"].fillna("") + " " +
        df_["AutoCategory"].fillna("")
    ).str.lower()

def apply_exclusions(df_, exclude_codes, exclude_topics):
    df = df_.copy()
    if not exclude_codes and not exclude_topics:
        return df
    df["__search__"] = build_search_text(df)
    mask = ~df["CourseCode"].str.upper().isin({c.upper() for c in exclude_codes})
    if exclude_topics:
        topic_kw = "|".join(map(re.escape, exclude_topics))
        mask &= ~df["__search__"].str.contains(topic_kw, case=False, na=False)
    return df[mask].drop(columns=["__search__"])

def _row_from_code_safe(code: str, uniq: dict, df_: pd.DataFrame):
    """
    å®‰å…¨åœ°é€šè¿‡è¯¾ç¨‹å·æ‹¿åˆ°ä¸€è¡Œè®°å½•ï¼š
    - å¦‚æœåœ¨ uniq å­—å…¸é‡Œæœ‰ï¼ˆæ’è¯¾æ—¶ç¼“å­˜çš„è¡Œï¼‰ï¼Œä¼˜å…ˆå–
    - å¦åˆ™ä» df_ é‡ŒæŒ‰ CourseCode ç²¾ç¡®åŒ¹é…
    """
    c = str(code).upper().strip()
    if isinstance(uniq, dict) and c in uniq:
        return uniq[c]
    hit = df_[df_["CourseCode"].str.upper().eq(c)]
    return hit.iloc[0] if not hit.empty else None


def _norm(s: str) -> str:
    return (str(s or "").strip().lower())

def _category_match(cat: str, tag_list: list) -> bool:
    c = _norm(cat)
    return any(t.lower() in c for t in tag_list if t)

# æ”¾åˆ°å¸¸é‡åŒºä»»æ„ä½ç½®ï¼Œä¾¿äºå¤ç”¨
CAPSTONE_CODES = set(PROJECT_ROUTE) | set(RESEARCH_ROUTE)

def _offered_in_term(row, term: str) -> bool:
    terms = str(row.get("OfferingTerms","") or "").upper().strip()
    if not terms:
        return True  # å…œåº•ï¼šæœªçŸ¥å­¦æœŸè§†ä¸ºå¯æ’ï¼ˆç”¨äºå­¦ä¹ è®¡åˆ’ï¼‰
    if "NOT OFFERED" in terms:
        return False
    return term in terms or terms in {"ALL", "ANY"}

def _parse_codes_list(s: str):
    s = (s or "").upper().replace("ï¼Œ",";").replace(",",";")
    parts = [p.strip() for p in re.split(r"[;|/ ]+", s) if p.strip()]
    return [p for p in parts if re.match(r"^[A-Z]{4}\d{4}$", p)]

def _prereq_codes(row) -> set:
    text = str(row.get("ConditionsForEnrolment","") or "")
    low = text.lower()
    # ä»…åœ¨æ˜ç¡®å‡ºç°â€œå…ˆä¿®/ä¿®è¯»è¦æ±‚â€æ—¶ï¼Œæ‰è¯†åˆ«å…ˆä¿®è¯¾
    if not re.search(r"prereq|pre-?requisite|assumed knowledge|must have completed|completion of", low):
        return set()
    # æå–è¯¾ç¨‹å·ï¼Œå¹¶é™åˆ¶åœ¨å¸¸è§å­¦é™¢å‰ç¼€
    codes = set(_parse_codes_list(text))
    allowed_prefixes = ("COMP","MATH","GSOE","ENGG","DATA","SENG","INFS")
    return {c for c in codes if c.startswith(allowed_prefixes)}


def _collect_bucket_courses(df_, bucket_cfg):
    # å…ˆæŒ‰ AutoCategory/Category åŒ¹é…ï¼ˆå°å†™åŒ…å«ï¼‰
    tags = [t.lower() for t in bucket_cfg["tags"]]
    def _cat_of_row(row):
        return str(row.get("AutoCategory", row.get("Category",""))).lower().strip()

    hit = df_[df_.apply(lambda r: any(t in _cat_of_row(r) for t in tags), axis=1)]

    # å…œåº•ï¼šæŒ‰â€œè¯¾ç¨‹å·é›†åˆâ€å¼ºåˆ¶è¡¥é½ï¼ˆè§£å†³ Category ä¸ºç©ºçš„é—®é¢˜ï¼‰
    if hit.empty:
        key = bucket_cfg["key"]
        if key == "found_core":
            hit = df_[df_["CourseCode"].str.upper().isin(FOUND_CORE)]
        elif key == "adv_core":
            hit = df_[df_["CourseCode"].str.upper().isin(ADV_CORE)]
        elif key == "ai_core":
            hit = df_[df_["CourseCode"].str.upper().isin(AI_CORE)]
        elif key == "dke":
            hit = df_[df_["CourseCode"].str.upper().isin(DKE_CODES)]
        elif key == "elective":
            hit = df_[df_["CourseCode"].str.upper().isin(ELECTIVE_CODES)]
        elif key == "project":
            hit = df_[df_["CourseCode"].str.upper().isin(PROJECT_ROUTE | RESEARCH_ROUTE)]

    # ä¸€èˆ¬é€‰ä¿®é‡Œæ’é™¤å¸¦â€œdisciplinaryâ€çš„ï¼Œé¿å…åƒæ‰ DKE
    if bucket_cfg["key"] == "elective":
        hit = hit[~hit.apply(lambda r: "disciplinary" in _cat_of_row(r), axis=1)]

    # å…œåº•ï¼šAI æ ¸å¿ƒå†ä¿è¯ä¸€ä¸‹
    if bucket_cfg["key"] == "ai_core" and hit.empty:
        hit = df_[df_["CourseCode"].str.upper().isin(["COMP9414","COMP9814"])]

    return hit.copy()

def _choose_courses_for_bucket(df_, bucket_cfg, already: set, prefer_topic="AI", exclude_codes=None, exclude_topics=None):
    cand = _collect_bucket_courses(df_, bucket_cfg)
    if exclude_codes is not None or exclude_topics is not None:
        cand = apply_exclusions(cand, exclude_codes or set(), exclude_topics or set())
    if cand.empty:
        return cand
    def score_row(r):
        s = 0
        desc = (str(r.get("Description","")) + " " + str(r.get("CourseName",""))).lower()
        if prefer_topic.lower() in desc: s += 3
        terms = str(r.get("OfferingTerms","") or "").upper()
        s += len([t for t in ["T1","T2","T3"] if t in terms])  # å¼€è¯¾è¶Šå¤šè¶Šå¥½
        eq = set(_parse_codes_list(r.get("EquivalentCourses","")))
        ex = set(_parse_codes_list(r.get("ExclusionCourses","")))
        if already & (eq | ex): s -= 10
        return s
    cand = cand[~cand["CourseCode"].str.upper().isin(already)].copy()
    if cand.empty:
        return cand
    cand["_score"] = cand.apply(score_row, axis=1)
    cand = cand.sort_values(["_score","CourseCode"], ascending=[False, True])
    return cand

def _select_capstone_combo(df_, prefer: str = "auto"):
    """
    è¿”å› (capstone_rows, route_type)ï¼š
    - Projectï¼šCOMP9900 + (GSOE9010 æˆ– GSOE9011) + 1 é—¨ DKEï¼ˆ18 UOCï¼‰
    - Researchï¼š
        * 9991 + 9993ï¼ˆ9993=12UOCï¼‰=> å…±18UOCï¼Œæ— éœ€DKE
        * è‹¥æ— 9993ï¼Œåˆ™ 9991 + 9992 + 1 é—¨ DKE => å…±18UOC
    prefer: 'project' / 'research' / 'auto'
    """
    def row_of(code):
        hit = df_[df_["CourseCode"].str.upper().eq(code)]
        return hit.iloc[0] if not hit.empty else None
    have = lambda c: not df_[df_["CourseCode"].str.upper().eq(c)].empty

    # Research ä¼˜å…ˆï¼ˆå½“ prefer=research æˆ– auto ä¸”èµ„æºå¯ç”¨ï¼‰
    if prefer in ("research", "auto"):
        if have("COMP9991") and have("COMP9993"):
            r1, r3 = row_of("COMP9991"), row_of("COMP9993")
            return [r1, r3], "Research è·¯çº¿ï¼ˆCOMP9991 + COMP9993ï¼Œæ— éœ€é¢å¤–DKEï¼‰"
        if have("COMP9991") and have("COMP9992"):
            r1, r2 = row_of("COMP9991"), row_of("COMP9992")
            dke_pool = df_[df_["CourseCode"].str.upper().isin(DKE_CODES)]
            dke_pool = dke_pool[~dke_pool["CourseCode"].str.upper().isin({"COMP9991","COMP9992"})]
            if dke_pool.empty:
                dke_pool = df_[
                    (df_["AutoCategory"].str.lower().str.contains("disciplinary")) &
                    (~df_["CourseCode"].str.upper().isin({"COMP9991","COMP9992"}))
                ]
            dke_row = dke_pool.iloc[0] if not dke_pool.empty else None
            rows = [r1, r2] + ([dke_row] if dke_row is not None else [])
            return rows, "Research è·¯çº¿ï¼ˆCOMP9991 + COMP9992 + è‡³å°‘1é—¨DKEï¼‰"

    # Project
    if have("COMP9900") and (have("GSOE9010") or have("GSOE9011")):
        p = row_of("COMP9900")
        g = row_of("GSOE9011") if have("GSOE9011") else row_of("GSOE9010")
        dke_pool = df_[df_["CourseCode"].str.upper().isin(DKE_CODES)]
        dke_pool = dke_pool[~dke_pool["CourseCode"].str.upper().isin({"COMP9900","GSOE9010","GSOE9011"})]
        if dke_pool.empty:
            dke_pool = df_[(df_["AutoCategory"].str.lower().str.contains("disciplinary")) &
                           (~df_["CourseCode"].str.upper().isin({"COMP9900","GSOE9010","GSOE9011"}))]
        dke_row = dke_pool.iloc[0] if not dke_pool.empty else None
        rows = [p, g] + ([dke_row] if dke_row is not None else [])
        return rows, "Project è·¯çº¿ï¼ˆCOMP9900 + GSOE9010/9011 + è‡³å°‘1é—¨DKEï¼‰"

    return [], "æœªç¡®å®š"

def _schedule_ai_plan(df_, rules=AI_RULES, completed: set = None, prefer_route: str = "auto", term_loads: list | None = None, exclude_codes: set | None = None, exclude_topics: set | None = None):
    """
    å¼¹æ€§å­¦æœŸè´Ÿè½½ + å…ˆä¿®3é—¨åŸºç¡€æ ¸å¿ƒ + å…¶ä½™ç©¿æ’ï¼š
    - å­¦æœŸè´Ÿè½½ï¼šç”¨æˆ·å¯æŒ‡å®šï¼ˆæ¯å­¦æœŸ1~3é—¨ï¼‰ï¼›å¦åˆ™é»˜è®¤ DEFAULT_LOADS
    - é˜¶æ®µ1ï¼šFoundational Coreï¼ˆå‡‘æ»¡18UOC=3é—¨ï¼‰ï¼Œå°½é‡æ”¾åˆ°æœ€å‰é¢çš„å­¦æœŸ
    - é˜¶æ®µ2ï¼šAdv Core + AI Core + DKE + Elective æŒ‰ä¼˜å…ˆçº§äº¤é”™æ”¾å…¥ï¼ˆä¸å¼ºåˆ¶â€œé€‰ä¿®å¿…é¡»æœ€åâ€ï¼‰
    - Capstone/Researchï¼šå…ˆé€‰ç»„åˆï¼Œä½†ä¼˜å…ˆå°è¯•æ”¾åœ¨åä¸¤å­¦æœŸï¼ˆè‹¥å…ˆä¿®/å¼€è¯¾ä¸å…è®¸ï¼Œä¼šè‡ªåŠ¨å‰ç§»ï¼‰
    - æ»¡è¶³éCapstone 78UOC + å„æ¡¶æœ€ä½å­¦åˆ†ï¼ˆFound18 / Adv18 / AI6 / DKE18 / Elective18ï¼‰
    """
    DEFAULT_LOADS = [3,3,2, 3,3,2]  # æ²¡æŒ‡å®šæ—¶çš„å¸¸ç”¨è´Ÿè½½
    loads = term_loads if (isinstance(term_loads, list) and len(term_loads)==6) else DEFAULT_LOADS
    # é˜²å¾¡ï¼šé™åˆ¶èŒƒå›´1~3
    loads = [min(3, max(1, int(x))) for x in loads]

    completed = set((completed or set()))
    taken_codes = set(code.upper() for code in completed)

    # 0) å…ˆé€‰ Capstone/Research ç»„åˆï¼ˆä¸ç«‹å³æ’è¯¾ï¼Œåªä½œä¸ºå€™é€‰ï¼‰
    cap_rows, route_type_hint = _select_capstone_combo(df_, prefer=prefer_route)
    cap_codes = [str(r["CourseCode"]).upper() for r in cap_rows if r is not None]

    # 1) ä¸ºéCapstone 5 æ¡¶æ„å»ºå€™é€‰æ± ï¼ˆå…ˆ Found â†’ å† Adv/AI/DKE/Electiveï¼‰
    taken_codes.update(cap_codes)
    chosen_by_bucket = {b["key"]: [] for b in rules["buckets"]}
    uoc_by_bucket = {b["key"]: 0 for b in rules["buckets"]}

    # é˜¶æ®µ1ï¼šåŸºç¡€æ ¸å¿ƒï¼ˆåªå–åˆ°18UOC=3é—¨ï¼‰
    for key in ["found_core"]:
        bcfg = next(b for b in rules["buckets"] if b["key"] == key)
        need_uoc = bcfg["uoc_min"]
        cand = _choose_courses_for_bucket(df_, bcfg, already=taken_codes, exclude_codes=exclude_codes or set(), exclude_topics=exclude_topics or set())
        if cand is not None and not cand.empty:
            for _, r in cand.iterrows():
                code = str(r["CourseCode"]).upper()
                if code in taken_codes:
                    continue
                chosen_by_bucket[key].append(r)
                uoc_by_bucket[key] += _uoc_of(r)
                taken_codes.add(code)
                if uoc_by_bucket[key] >= need_uoc:
                    break

    # é˜¶æ®µ2ï¼šå…¶ä½™æ¡¶ä¾æ¬¡è¡¥é½åˆ°â€œæœ€ä½å­¦åˆ†â€ï¼Œä»ï¼šé«˜çº§æ ¸å¿ƒ â†’ AIæ ¸å¿ƒ â†’ DKE â†’ Elec ä½†æ˜¯åªæœ‰26å¹´ éš¾å—
    phase2_order = ["adv_core","ai_core","dke","elective"]
    for key in phase2_order:
        bcfg = next(b for b in rules["buckets"] if b["key"] == key)
        need_uoc = bcfg["uoc_min"]
        cand = _choose_courses_for_bucket(df_, bcfg, already=taken_codes, exclude_codes=exclude_codes or set(), exclude_topics=exclude_topics or set())
        if cand is None or cand.empty:
            continue
        for _, r in cand.iterrows():
            code = str(r["CourseCode"]).upper()
            if code in taken_codes:
                continue
            chosen_by_bucket[key].append(r)
            uoc_by_bucket[key] += _uoc_of(r)
            taken_codes.add(code)
            if uoc_by_bucket[key] >= need_uoc:
                break

    # é˜¶æ®µ3ï¼šå¦‚éCapstone < 78UOCï¼Œå†ä» DKE â†’ Elective ç»§ç»­è¡¥ï¼Œç›´åˆ° 78UOC
    def total_noncap_uoc():
        return sum(_uoc_of(x) for ks in ["found_core","adv_core","ai_core","dke","elective"]
                   for x in chosen_by_bucket[ks])
    for key in ["dke","elective","adv_core","ai_core"]:  # å†ç»™äº›å›æ—‹ç©ºé—´
        if total_noncap_uoc() >= NON_CAPSTONE_TARGET_UOC: break
        bcfg = next(b for b in rules["buckets"] if b["key"] == key)
        cand = _choose_courses_for_bucket(df_, bcfg, already=taken_codes, exclude_codes=exclude_codes or set(), exclude_topics=exclude_topics or set())
        if cand is None or cand.empty:
            continue
        for _, r in cand.iterrows():
            if total_noncap_uoc() >= NON_CAPSTONE_TARGET_UOC: break
            code = str(r["CourseCode"]).upper()
            if code in taken_codes:
                continue
            chosen_by_bucket[key].append(r)
            taken_codes.add(code)

    # 2) æ’æœŸï¼šå…ˆ Foundï¼ˆä¸‰é—¨å°½é‡æ”¾æœ€å‰ï¼‰
    terms = ["T1","T2","T3","T1","T2","T3"]
    schedule = {i+1: [] for i in range(6)}
    term_cap = loads[:]               # æ¯å­¦æœŸæœ€å¤šé—¨æ•°
    placed = set(completed)

    def try_place(course_row, term_idx):
        if term_cap[term_idx] <= 0:
            return False
        code = str(course_row["CourseCode"]).upper()
        if code in placed:
            return False
        prereq = _prereq_codes(course_row)
        if not prereq.issubset(placed):
            return False
        if not _offered_in_term(course_row, terms[term_idx]):
            return False
        schedule[term_idx+1].append(code)
        placed.add(code)
        term_cap[term_idx] -= 1
        return True

    # æ”¾ç½®é¡ºåº Aï¼šFoundï¼ˆå°½é‡ 0â†’1â†’2â†’3â†’4â†’5ï¼‰
    early_order = [0,1,2,3,4,5]
    for r in chosen_by_bucket.get("found_core", []):
        for ti in early_order:
            if try_place(r, ti): break

    # æ”¾ç½®é¡ºåº Bï¼šå…¶ä½™æ¡¶ç©¿æ’ï¼ˆä¼˜å…ˆçº§ï¼šAdv â†’ AI â†’ DKE â†’ Electiveï¼‰ï¼Œä»ä»¥æ—©å­¦æœŸä¼˜å…ˆ
    for key in ["adv_core","ai_core","dke","elective"]:
        for r in chosen_by_bucket.get(key, []):
            for ti in early_order:
                if try_place(r, ti): break

    # æ”¾ç½®é¡ºåº Cï¼šCapstoneï¼ˆå°½é‡ 5â†’4â†’3â†’2â†’1â†’0ï¼‰
    cap_order = [5,4,3,2,1,0]
    for r in cap_rows:
        if r is None:
            continue
        for ti in cap_order:
            if try_place(r, ti):
                break

    # 3) ç¼ºå£ç»Ÿè®¡ï¼ˆåŒ v4.4 å£å¾„ï¼‰
    uniq = {}
    for i in range(1, 7):
        for c in schedule[i]:
            if c not in uniq:
                hit = df_[df_["CourseCode"].str.upper().eq(c)]
                if not hit.empty:
                    uniq[c] = hit.iloc[0]

    flat = [c for term in schedule.values() for c in term]
    planned_set = set(flat)
    bucket_tags = {b["key"]: b["tags"] for b in rules["buckets"]}
    missing = []

    cap_planned = planned_set & set(cap_codes)
    non_cap_planned = planned_set - cap_planned

    # å„æ¡¶ä¸‹é™
    for key in ["found_core","adv_core","ai_core","dke","elective"]:
        want = next(b for b in rules["buckets"] if b["key"]==key)["uoc_min"]
        got = 0
        for c in non_cap_planned:
            row = _row_from_code_safe(c, uniq, df_)
            if row is None:
                continue
            if _category_match(str(row.get("AutoCategory", row.get("Category",""))), bucket_tags[key]):
                got += _uoc_of(row)
        if got < want:
            missing.append((key, want - got))

    # éCap æ€» UOC
    noncap_total = 0
    for c in non_cap_planned:
        row = _row_from_code_safe(c, uniq, df_)
        if row is not None:
            noncap_total += _uoc_of(row)
    if noncap_total < NON_CAPSTONE_TARGET_UOC:
        missing.append(("noncap_total", NON_CAPSTONE_TARGET_UOC - noncap_total))

    # Capstone UOCï¼ˆç›®æ ‡ 18ï¼‰
    cap_total = 0
    for c in cap_planned:
        row = _row_from_code_safe(c, uniq, df_)
        if row is not None:
            cap_total += _uoc_of(row)
    if cap_total < CAPSTONE_TARGET_UOC:
        missing.append(("project", CAPSTONE_TARGET_UOC - cap_total))

    # 4) è·¯çº¿ç±»å‹ï¼ˆä»¥æœ€ç»ˆæ’å…¥çš„è¯¾ç¨‹ä¸ºå‡†ï¼‰
    final_route = "æœªç¡®å®š"
    all_codes = set([c.upper() for term in schedule.values() for c in term])
    if {"COMP9900"} & all_codes and ({"GSOE9010"} & all_codes or {"GSOE9011"} & all_codes):
        final_route = "Project è·¯çº¿ï¼ˆCOMP9900 + GSOE9010/9011 + è‡³å°‘1é—¨DKEï¼‰"
    elif {"COMP9991"} & all_codes:
        if {"COMP9993"} & all_codes:
            final_route = "Research è·¯çº¿ï¼ˆCOMP9991 + COMP9993ï¼Œæ— éœ€é¢å¤–DKEï¼‰"
        elif {"COMP9992"} & all_codes:
            final_route = "Research è·¯çº¿ï¼ˆCOMP9991 + COMP9992 + è‡³å°‘1é—¨DKEï¼‰"

    return schedule, terms, missing, final_route

# =========================================================
# Intent detection / Chitchat / Nodes
# =========================================================
def detect_intent_node(state):
    latest = _extract_latest_user_utterance(state.get("query", ""))
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯¾ç¨‹åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·è¾“å…¥åˆ¤æ–­å…¶æ„å›¾ã€‚
è¾“å‡º JSONï¼š
{{
  "intent": "chitchat / term_query / recommend / plan / search",
  "course_code": "(å¦‚æœæåˆ°è¯¾ç¨‹å·)",
  "topic": "(å¦‚AIã€æ•°æ®ã€ç¼–ç¨‹)",
  "num_courses": "(æ•°å­—æˆ–None)"
}}
åªè¾“å‡ºJSONï¼Œä¸è¦è§£é‡Šã€‚
ç”¨æˆ·è¾“å…¥ï¼šã€Œ{latest}ã€
""".strip()
    intent = {"intent": "search", "course_code": None, "topic": None, "num_courses": None}
    try:
        result = Generation.call(model=LLM_MODEL, prompt=prompt)
        output = extract_text(result).strip()
        try:
            parsed = json.loads(output)
            if isinstance(parsed, dict) and "output" in parsed:
                inner_text = parsed["output"].get("text", "")
                if inner_text.strip().startswith("{"):
                    parsed = json.loads(inner_text)
        except Exception:
            parsed = None
        if isinstance(parsed, dict):
            intent.update(parsed)
    except Exception:
        pass
    return {
        "intent": intent.get("intent", "search"),
        "course_code": intent.get("course_code"),
        "topic": intent.get("topic"),
        "num_courses": intent.get("num_courses"),
        "detail_type": "",
        "route_pref": state.get("route_pref","auto"),
        "next_node": intent.get("intent", "search"),
    }

def chitchat_node(state):
    raw = state["query"]
    query = _extract_latest_user_utterance(raw)
    user_lower = query.lower()
    if any(g in user_lower for g in ["ä½ å¥½","hello","hi","å—¨","åœ¨å—","thanks","è°¢è°¢","æ—©ä¸Šå¥½","ä¸‹åˆå¥½","æ™šä¸Šå¥½"]):
        return safe_answer("ğŸ˜Š æˆ‘åœ¨ï½æƒ³äº†è§£å“ªé—¨è¯¾ï¼Ÿ")
    prompt = f"""
ä½ æ˜¯ä¸€ä½å‹å¥½ã€è½»æ¾ã€è¯­æ°”è‡ªç„¶çš„å¤§å­¦åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ï¼Œç”¨ä¸€å¥ä¸­æ–‡è‡ªç„¶å›å¤ï¼šå£è¯­åŒ–ã€äº²åˆ‡ã€ä¸å•°å—¦ã€ä¸æ¢è¡Œã€‚å…è®¸è‡³å¤šä¸€ä¸ªè¡¨æƒ…ã€‚
ç”¨æˆ·è¯´ï¼šã€Œ{query}ã€
ç°åœ¨å›å¤ï¼š
""".strip()
    try:
        result = Generation.call(model=LLM_MODEL, prompt=prompt)
        text = extract_text(result).strip() or "ğŸ¤– æˆ‘åœ¨å‘¢ï½æƒ³äº†è§£å“ªé—¨è¯¾ï¼Ÿ"
    except Exception:
        text = "ğŸ¤– æˆ‘åœ¨å‘¢ï½æƒ³äº†è§£å“ªé—¨è¯¾ï¼Ÿ"
    return safe_answer(text)

def search_course_node(state):
    raw_query = state.get("query", "")
    query = _extract_latest_user_utterance(raw_query)
    row = lookup_relaxed(query)
    if row is not None:
        r = row
        info = [f"ğŸ“˜ {r['CourseCode']} - {r.get('CourseName','')} ({r.get('Credits','')}UOC, {r.get('OfferingTerms','')})"]
        if r.get("AutoCategory") or r.get("Category"):
            info.append(f"ğŸ“‚ ç±»åˆ«ï¼š{r.get('AutoCategory') or r.get('Category')}")
        if r.get("ConditionsForEnrolment"): info.append(f"ğŸ”‘ å‰ç½®ï¼š{r['ConditionsForEnrolment']}")
        if r.get("EquivalentCourses"): info.append(f"ğŸ” ç­‰ä»·ï¼š{r['EquivalentCourses']}")
        if r.get("ExclusionCourses"): info.append(f"ğŸš« äº’æ–¥ï¼š{r['ExclusionCourses']}")
        if r.get("Description"): info.append(f"ğŸ“ {r['Description']}")
        return safe_answer("\n".join(info))
    if vectorstore is not None:
        results = vectorstore.similarity_search(query, k=TOP_K)
        if results:
            answer = [f"{i+1}. ğŸ“˜ {r.page_content}" for i, r in enumerate(results)]
            return safe_answer(answer)
    return safe_answer("âŒ æ²¡æ‰¾åˆ°ç›¸å…³è¯¾ç¨‹ã€‚")

def term_query_node(state):
    raw_query = state.get("query", "")
    query = _extract_latest_user_utterance(raw_query)
    rows = lookup_strict(query)
    if rows is None or rows.empty:
        recent = _recent_codes_from_context(raw_query, limit=1)
        if recent:
            rows = df[df["CourseCode"].str.upper().isin([c.upper() for c in recent])]
        if rows is None or rows.empty:
            return safe_answer("âŒ æœªè¯†åˆ«åˆ°è¯¾ç¨‹å·ã€‚")
    outputs = []
    for _, r in rows.iterrows():
        code = str(r.get("CourseCode", "")).strip()
        terms = str(r.get("OfferingTerms", "")).strip() or "N/A"
        if terms.lower() in ["not offered", "nan", "none", "", "n/a"]:
            outputs.append(f"âš ï¸ {code} å½“å‰æœªåœ¨å®˜æ–¹Termåˆ—è¡¨ä¸­å¼€è®¾ã€‚")
        else:
            outputs.append(f"ğŸ“… {code} åœ¨ {terms} å¼€è¯¾ã€‚")
    return safe_answer("\n".join(outputs))

def detail_query_node(state):
    raw_query = state.get("query", "")
    detail = state.get("detail_type", "")
    query = _extract_latest_user_utterance(raw_query)
    rows = lookup_strict(query)
    if rows is None or rows.empty:
        recent = _recent_codes_from_context(raw_query, limit=3)
        if recent:
            rows = df[df["CourseCode"].str.upper().isin([c.upper() for c in recent])]
    if rows is None or rows.empty:
        _, nums = _extract_candidates_from_query(query)
        hint = f"ï¼ˆè¯·å°è¯•å®Œæ•´å†™æ³•ï¼Œå¦‚ COMP{nums[0]}ï¼‰" if nums else ""
        return safe_answer(f"âŒ æ²¡æ‰¾åˆ°ç›¸å…³è¯¾ç¨‹ã€‚{hint}".strip())
    outputs = []
    for _, r in rows.iterrows():
        code = str(r.get("CourseCode", "")).strip()
        if detail == "prereq":
            val = str(r.get("ConditionsForEnrolment", "")).strip() or "ï¼ˆæœªåœ¨Handbookæ˜ç¡®ç»™å‡ºï¼‰"
            outputs.append(f"ğŸ”‘ {code} çš„å‰ç½®/é™ä¿®ï¼š{val}")
        elif detail == "exclusion":
            val = str(r.get("ExclusionCourses", "")).strip() or "ï¼ˆæ— ï¼‰"
            outputs.append(f"ğŸš« {code} çš„äº’æ–¥è¯¾ç¨‹ï¼š{val}")
        elif detail == "equivalent":
            val = str(r.get("EquivalentCourses", "")).strip() or "ï¼ˆæ— ï¼‰"
            outputs.append(f"ğŸ” {code} çš„ç­‰ä»·è¯¾ç¨‹ï¼š{val}")
        elif detail == "category":
            val = str(r.get("AutoCategory", r.get("Category",""))).strip() or "ï¼ˆæœªæ ‡æ³¨ç±»åˆ«ï¼‰"
            outputs.append(f"ğŸ“‚ {code} å±äºï¼š{val}")
        elif detail == "desc":
            name = str(r.get("CourseName","")).strip()
            desc = str(r.get("Description","")).strip() or "ï¼ˆæš‚æ— æè¿°ï¼‰"
            outputs.append(f"ğŸ“ {code}{' - ' + name if name else ''} çš„è¯¾ç¨‹æè¿°ï¼š{desc}")
        else:
            parts = [
                f"ğŸ“˜ {code} - {str(r.get('CourseName',''))} ({str(r.get('Credits',''))}UOC, {str(r.get('OfferingTerms',''))})",
            ]
            if str(r.get("AutoCategory", r.get("Category",""))).strip():
                parts.append(f"ğŸ“‚ ç±»åˆ«ï¼š{r.get('AutoCategory', r.get('Category',''))}")
            if str(r.get("ConditionsForEnrolment","")).strip():
                parts.append(f"ğŸ”‘ å‰ç½®ï¼š{r.get('ConditionsForEnrolment')}")
            if str(r.get("EquivalentCourses","")).strip():
                parts.append(f"ğŸ” ç­‰ä»·ï¼š{r.get('EquivalentCourses')}")
            if str(r.get("ExclusionCourses","")).strip():
                parts.append(f"ğŸš« äº’æ–¥ï¼š{r.get('ExclusionCourses')}")
            if str(r.get("Description","")).strip():
                parts.append(f"ğŸ“ æè¿°ï¼š{r.get('Description')}")
            outputs.append("\n".join(parts))
    return safe_answer("\n".join(outputs))

# â€”â€” æ¨è / ç®€å•Planï¼ˆä¿ç•™ï¼‰
CATEGORY_RANK = {
    "Foundational Core Courses": 1,
    "Core Courses": 2,
    "Prescribed Electives": 3,
    "Disciplinary Prescribed Electives": 3,
    "Advanced Disciplinary Electives": 3,
    "Disciplinary Electives": 4,
    "General Education": 5,
    "Capstone": 6,
    "Research": 7,
    "Other": 8,
}
USER_COMPLETED: set = set()

def recommend_course_node(state):
    raw = state["query"]
    query_latest = _extract_latest_user_utterance(raw)
    query_upper = str(query_latest).upper()
    topic = state.get("topic") or "AI"
    num = state.get("num_courses")
    try:
        num = int(num)
    except:
        m = re.search(r"(?:æ¨è|æ¥|ç»™æˆ‘|è¦)?\s*(\d{1,3})\s*(?:é—¨|ä¸ª)?", query_upper)
        num = int(m.group(1)) if m else 5
    num = max(1, min(num, 50))
    term = None
    for t in ["T1","T2","T3"]:
        if t in query_upper: term = t; break
    cand = df[df["OfferingTerms"].str.upper().str.contains(term, na=False)] if term else df.copy()
    if term: cand = cand[~cand["OfferingTerms"].str.contains("NOT OFFERED", na=False)]
    if cand.empty:
        return safe_answer(f"âš ï¸ å½“å‰æœªæ‰¾åˆ°åœ¨ {term} å¼€è®¾çš„è¯¾ç¨‹ã€‚")
    def score_row(r: pd.Series) -> float:
        cat = str(r.get("AutoCategory", r.get("Category","")))
        base = 100 - CATEGORY_RANK.get(cat, 9) * 10
        desc = (((r.get("Description","") + " " + r.get("CourseName",""))).lower())
        topic_hit = 5 if topic.lower() in desc else 0
        penalty = 0
        eq = set(_parse_codes_list(r.get("EquivalentCourses","")))
        ex = set(_parse_codes_list(r.get("ExclusionCourses","")))
        if USER_COMPLETED & (eq | ex): penalty += 50
        cond = (r.get("ConditionsForEnrolment","") or "").lower()
        if re.search(r"prereq|pre-?requisite|requirement|assumed", cond): penalty += 2
        return base + topic_hit - penalty
    cand = cand.copy()
    cand["_score"] = cand.apply(score_row, axis=1)
    cand = cand.sort_values(["_score", "CourseCode"], ascending=[False, True])
    selected = cand.head(num)
    items = []
    for i, r in enumerate(selected.itertuples(index=False), 1):
        short_desc = re.sub(r"\s+", " ", str(getattr(r, "Description", ""))).strip()
        if len(short_desc) > 110: short_desc = short_desc[:110] + "..."
        eq = getattr(r, "EquivalentCourses", ""); ex = getattr(r, "ExclusionCourses", ""); cond = getattr(r, "ConditionsForEnrolment", "")
        tags = []
        cat = getattr(r, "AutoCategory", "") or getattr(r, "Category", "")
        if cat: tags.append(cat)
        if term: tags.append(term)
        meta = " | ".join(tags)
        lines = [
            f"{i}. ğŸ“˜ {r.CourseCode} - {getattr(r,'CourseName','')} ({getattr(r,'Credits','')}UOC, {getattr(r,'OfferingTerms','')})",
            f"   ğŸ·ï¸ {meta}" if meta else "",
            f"   ğŸ“ {short_desc}" if short_desc else "",
            f"   ğŸ”‘ å‰ç½®ï¼š{cond}" if cond else "",
            f"   ğŸ” ç­‰ä»·ï¼š{eq}" if eq else "",
            f"   ğŸš« äº’æ–¥ï¼š{ex}" if ex else "",
        ]
        items.append("\n".join([x for x in lines if x]))
    header = f"ğŸ¯ ä¸ºä½ æ¨èçš„ {term or topic} è¯¾ç¨‹ï¼ˆå…± {len(items)} é—¨ï¼‰ï¼š"
    return safe_answer([header] + items)

def plan_course_node(state):
    topic = state.get("topic") or "AI"
    selected = df[df["Description"].str.lower().str.contains(topic.lower(), na=False)]
    if selected.empty:
        return safe_answer(f"âŒ æœªæ‰¾åˆ°ä¸ {topic} ç›¸å…³çš„è¯¾ç¨‹ã€‚")
    selected["_rank"] = selected["Category"].map(lambda c: CATEGORY_RANK.get(str(c), 9))
    plan = selected.sort_values(["_rank", "CourseCode"]).head(6)
    plan_text = [f"{i+1}. {r.CourseCode} - {r.CourseName} ({r.OfferingTerms})" for i, r in plan.iterrows()]
    return safe_answer([f"ğŸ§  {topic.upper()} æ–¹å‘å»ºè®®ä¿®è¯»é¡ºåºï¼ˆä¼˜å…ˆæ ¸å¿ƒï¼‰ï¼š"] + plan_text)

# === Explain Panel Helpers ===
def _lookup_course(code: str) -> pd.Series | None:
    cc = str(code or "").upper().strip()
    hit = df[df["CourseCode"].str.upper()==cc]
    return hit.iloc[0] if not hit.empty else None

def _explain_course_line(code: str) -> str:
    r = _lookup_course(code)
    if r is None: return f"{code}"
    cat = (r.get("AutoCategory") or r.get("Category") or "").strip()
    term = str(r.get("OfferingTerms","")).strip()
    cond = str(r.get("ConditionsForEnrolment","")).strip()
    eq = str(r.get("EquivalentCourses","")).strip()
    ex = str(r.get("ExclusionCourses","")).strip()
    bits = [f"{code}"]
    if cat: bits.append(f"ç±»åˆ«:{cat}")
    if term: bits.append(f"å¼€è¯¾:{term}")
    if cond: bits.append(f"å…ˆä¿®:{cond}")
    if eq: bits.append(f"ç­‰ä»·:{eq}")
    if ex: bits.append(f"äº’æ–¥:{ex}")
    return "ï¼›".join(bits)

def build_explain_panel(schedule: dict[int, list[str]], terms: list[str], state: dict) -> str:
    lines = []
    # åå¥½å›æ˜¾
    ex_codes = [c.upper() for c in state.get("exclude_codes", [])]
    ex_topics = list(state.get("exclude_topics", []))
    if ex_codes or ex_topics:
        parts = []
        if ex_codes: parts.append("æ’é™¤è¯¾ç¨‹ï¼š" + "ã€".join(ex_codes))
        if ex_topics: parts.append("æ’é™¤ä¸»é¢˜ï¼š" + "ã€".join(ex_topics))
        lines.append("ğŸ§  åå¥½è®°å¿†ï¼š" + "ï¼›".join(parts))
    # æ¯å­¦æœŸè§£é‡Š
    for i, term in enumerate(terms, 1):
        xs = schedule.get(i, [])
        if not xs: continue
        lines.append(f"â€¢ {term}:")
        for code in xs:
            lines.append("   - " + _explain_course_line(code))
    # å¼•ç”¨ï¼šæ¥è‡ª CSV æ•°æ®æº
    lines.append("ğŸ”— å¼•ç”¨ï¼šè¯¾ç¨‹åç§°/å…ˆä¿®/ç­‰ä»·/äº’æ–¥/å¼€è¯¾å­¦æœŸå‡æ¥è‡ªæœ¬åœ° CSVï¼ˆ" + os.path.basename(CSV_FILE) + "ï¼‰ã€‚")
    return "\n".join(lines)
def grad_plan_node(state):
    global LAST_PLAN_SCHEDULE, LAST_PLAN_TERMS, LAST_PLAN_STATE
    # On-demand Explain: only show explanation for the last generated plan
    if state.get("explain_only"):
        if LAST_PLAN_SCHEDULE is None or LAST_PLAN_TERMS is None:
            return safe_answer("ğŸ“ è¿˜æ²¡æœ‰å¯è§£é‡Šçš„è®¡åˆ’ã€‚è¯·å…ˆè®©æˆ‘ç”Ÿæˆä¸€æ¬¡é€‰è¯¾å»ºè®®ï¼Œå†è¯´â€œè¯·ç»™è§£é‡Šâ€ã€‚")
        explain = build_explain_panel(LAST_PLAN_SCHEDULE, LAST_PLAN_TERMS, LAST_PLAN_STATE)
        return safe_answer("ğŸ§¾ è§£é‡Šä¸å¼•ç”¨\n" + explain)
    raw = state.get("query","")
    latest = _extract_latest_user_utterance(raw)

    # å·²ä¿®è¯¾ç¨‹ï¼šæ”¯æŒ COMP9021 / è£¸æ•°å­— 9021 => COMP9021
    completed = _parse_completed_codes(latest)

    # å­¦æœŸè´Ÿè½½ï¼šè‹¥ç”¨æˆ·å†™äº† "332 332" ä¹‹ç±»ï¼Œå°±ç”¨ä¹‹ï¼›å¦åˆ™äº¤ç»™æ’è¯¾å™¨é»˜è®¤
    term_loads = _parse_term_loads(latest)

    prefer = state.get("route_pref","auto")
    schedule, terms, missing, route_type = _schedule_ai_plan(
        df, AI_RULES, completed=completed, prefer_route=prefer, term_loads=term_loads,
        exclude_codes=set(state.get("exclude_codes", set())), exclude_topics=set(state.get("exclude_topics", set()))
    )

    lines = ["ğŸ“š AIæ–¹å‘ä¸¤å¹´å­¦ä¹ å»ºè®®ï¼ˆè‰æ¡ˆï¼‰"]
    for i, t in enumerate(terms, 1):
        xs = schedule[i]
        lines.append(f"{i}. {t}ï¼š " + ("ï¼Œ".join(xs) if xs else "ï¼ˆæš‚æœªå®‰æ’ï¼‰"))

    if missing:
        name_map = {"found_core": "åŸºç¡€æ ¸å¿ƒè¯¾","adv_core": "é«˜çº§æ ¸å¿ƒè¯¾","ai_core": "AIæ ¸å¿ƒè¯¾","dke": "DKE","elective": "ä¸€èˆ¬é€‰ä¿®","project": "æ¯•ä¸šç ”ç©¶/é¡¹ç›®","noncap_total": "éCapstoneå­¦åˆ†æ€»é‡"}
        warn = "ï¼›".join([f"{name_map.get(k,k)} ä»ç¼ºçº¦ {u} å­¦åˆ†" for k,u in missing])
        lines.append(f"\nâš ï¸ è¦†ç›–æç¤ºï¼š{warn}ã€‚å¯è¡¥å……åŒç±»è¯¾ç¨‹æˆ–è°ƒæ•´å­¦æœŸä»¥æ»¡è¶³æ¯•ä¸šæ ‡å‡†ã€‚")

    if route_type != "æœªç¡®å®š":
        lines.append(f"\nğŸ å½“å‰è§„åˆ’è·¯çº¿ï¼š{route_type}")

    if term_loads:
        lines.append(f"\nğŸ§© å­¦æœŸè´Ÿè½½é‡‡ç”¨ï¼š{'-'.join(map(str, term_loads[:3]))} / {'-'.join(map(str, term_loads[3:]))}ï¼ˆæ¯å­¦æœŸæœ€å¤š3é—¨ï¼Œæœ€å°‘1é—¨ï¼‰")

    lines.append("\nğŸ” è¯´æ˜ï¼šä¼˜å…ˆå®‰æ’ 3 é—¨åŸºç¡€æ ¸å¿ƒè¯¾ï¼Œå…¶ä½™ï¼ˆé«˜çº§æ ¸å¿ƒ/AIæ ¸å¿ƒ/DKE/ä¸€èˆ¬é€‰ä¿®ï¼‰å¯ç©¿æ’ï¼›"
                 "Capstone/Research åœ¨å…ˆä¿®å…è®¸æ—¶å°½é‡å®‰æ’åˆ°åä¸¤å­¦æœŸï¼›ä¸¥æ ¼éµå®ˆ CSV çš„å¼€è¯¾å­¦æœŸã€‚")

    # Cache the latest plan for on-demand explanation
    LAST_PLAN_SCHEDULE = schedule
    LAST_PLAN_TERMS = terms
    LAST_PLAN_STATE = dict(state)
    return safe_answer("\n".join(lines))


def _extract_codes_from_text(s: str) -> list[str]:
    import re as _re
    text = str(s or "")

    # 1) å…ˆæŠ“å®Œæ•´è¯¾å·ï¼ˆå®¹å¿ç©ºæ ¼/è¿å­—ç¬¦ï¼‰ï¼šCOMP9414 / COMP-9414 / COMP 9414
    full = _re.findall(r"(?<![A-Z0-9])([A-Z]{4}\s*-?\s*\d{4})(?![A-Z0-9])", text.upper())
    full = [_re.sub(r"[\s-]+","",c) for c in full]

    # 2) å†æŠ“ 4 ä½æ•°å­—ç®€å†™ï¼š9414 / 9814
    short = _re.findall(r"(?<!\d)(\d{4})(?!\d)", text)
    mapped = []
    for d in short:
        try:
            # åœ¨ df é‡Œæ‰¾â€œè¯¾ç¨‹ä»£ç ä»¥è¯¥ 4 ä½ç»“å°¾â€çš„å€™é€‰
            candidates = df[df["CourseCode"].str.endswith(d)]["CourseCode"].str.upper().unique().tolist()
        except Exception:
            candidates = []
        # æœ‰ COMP ä¼˜å…ˆå– COMPï¼›å¦åˆ™å”¯ä¸€å°±å–å”¯ä¸€ï¼›å¤šäºä¸€ä¸ªå°±å…ˆå–ç¬¬ä¸€ä¸ªï¼ˆéœ€è¦çš„è¯å¯åšæ¶ˆæ­§æç¤ºï¼‰
        comp_first = [c for c in candidates if c.startswith("COMP")]
        if comp_first:
            mapped.append(comp_first[0])
        elif len(candidates) == 1:
            mapped.append(candidates[0])
        elif len(candidates) > 1:
            mapped.append(candidates[0])

    # 3) åˆå¹¶å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
    out, seen = [], set()
    for c in full + mapped:
        if c and c not in seen:
            seen.add(c)
            out.append(c)
    return out

def reviews_node(state):
    q = _extract_latest_user_utterance(state.get("query",""))
    codes = _extract_codes_from_text(q)
    import re as _re
    # å¯¹æ¯”åœºæ™¯
    if len(codes) >= 2 and _re.search(r"(å¯¹æ¯”|åŒºåˆ«|diff|compare|å“ªä¸ªå¥½|æ›´æ¨è|vs|æ¯”è¾ƒ)", q.lower()):
        a, b = codes[:2]
        return safe_answer(compare_reviews(a, b))
    # å•è¯¾è¯„ä»·
    if len(codes) >= 1:
        return safe_answer(summarize_reviews_for(codes[0]))
    # æ²¡æŠ“åˆ°è¯¾ç¨‹å·ï¼šæç¤ºç”¨æˆ·æä¾›
    return safe_answer("æƒ³çœ‹å“ªé—¨è¯¾çš„å£ç¢‘ï¼Ÿè¯·å¸¦ä¸Šè¯¾ç¨‹å·ï¼ˆå¦‚ COMP9414ï¼‰ã€‚\nä¾‹å¦‚ï¼šCOMP9414 è¯„ä»·æ€ä¹ˆæ ·ï¼Ÿ")

# === Export Node ===
def export_node(state):
    path = state.get("export_path") or "plan.csv"
    try:
        _has = (LAST_PLAN_SCHEDULE is not None) and (LAST_PLAN_TERMS is not None)
    except NameError:
        _has = False
    if not _has:
        return safe_answer("ğŸ“ è¿˜æ²¡æœ‰å¯å¯¼å‡ºçš„è®¡åˆ’ã€‚è¯·å…ˆè®©æˆ‘ç”Ÿæˆä¸€æ¬¡é€‰è¯¾å»ºè®®ï¼Œå†è¯´â€œå¯¼å‡ºè®¡åˆ’â€ã€‚")
    try:
        out = export_plan_csv(LAST_PLAN_SCHEDULE, LAST_PLAN_TERMS, df, out_path=path, include_desc=True)
        return safe_answer(f"âœ… å·²å¯¼å‡ºï¼š{out}\nåŒ…å«åˆ—ï¼šTerm, CourseCode, CourseName, Description")
    except Exception as e:
        return safe_answer(f"âš ï¸ å¯¼å‡ºå¤±è´¥ï¼š{e}")

# === Export ICS Node ===
def export_ics_node(state):
    path = state.get("export_path") or "plan.ics"
    try:
        _has = (LAST_PLAN_SCHEDULE is not None) and (LAST_PLAN_TERMS is not None)
    except NameError:
        _has = False
    if not _has:
        return safe_answer("ğŸ“ è¿˜æ²¡æœ‰å¯å¯¼å‡ºçš„è®¡åˆ’ã€‚è¯·å…ˆè®©æˆ‘ç”Ÿæˆä¸€æ¬¡é€‰è¯¾å»ºè®®ï¼Œå†è¯´â€œå¯¼å‡ºæ—¥å†â€ã€‚")
    try:
        out = export_plan_ics(LAST_PLAN_SCHEDULE, LAST_PLAN_TERMS, df, out_path=path)
        return safe_answer(f"âœ… å·²å¯¼å‡ºï¼š{out}\nç”¨æ³•ï¼šåœ¨ Google/Apple/Outlook æ—¥å†ä¸­å¯¼å…¥ .ics å³å¯ï¼ˆAll-day äº‹ä»¶ï¼Œæ¯é—¨è¯¾ä¸€æ¡ï¼‰ã€‚")
    except Exception as e:
        return safe_answer(f"âš ï¸ å¯¼å‡ºå¤±è´¥ï¼š{e}")

# =========================================================
# Routerï¼ˆå«â€œç ”ç©¶/é¡¹ç›®è·¯çº¿â€åå¥½ã€æè¿°è¯†åˆ«ï¼‰
# =========================================================
def router_node(state):
    q = _extract_latest_user_utterance(state.get("query", ""))
    q_low = q.lower()
    if re.search(r"(å¯¼å‡º|ä¿å­˜).*(è®¡åˆ’|csv)|\bexport\b.*\bcsv\b|å¯¼å‡ºcsv|å¯¼å‡ºè¯¾ç¨‹è¡¨", q_low):
        return {"next_node": "export"}
    if re.search(r"(å¯¼å‡º|ä¿å­˜).*(æ—¥å†|ics)|\bexport\b.*\bics\b|å¯¼å‡ºæ—¥å†|å¯¼å‡ºics", q_low):
        return {"next_node": "export_ics"}
    # è¯¾ç¨‹è¯„ä»·/å£ç¢‘/éš¾åº¦/å·¥ä½œé‡/å¯¹æ¯” â†’ reviews
    _codes_in_q = re.findall(r"(?<![A-Z0-9])([A-Z]{4}\d{4})(?![A-Z0-9])", q.upper())
    if re.search(r"(è¯„ä»·|å£ç¢‘|æµ‹è¯„|review|reviews|éš¾åº¦|workload|ä½œä¸šå¤šä¸å¤š|ä½œä¸š|å“ªä¸ªå¥½|æ›´æ¨è|å¯¹æ¯”|åŒºåˆ«|æ¯”è¾ƒ|vs)", q_low):
        return {"next_node": "reviews"}
    # On-demand explain trigger
    if re.search(r"(ç»™.*è§£é‡Š|è¯·ç»™è§£é‡Š|è§£é‡Šä¸å¼•ç”¨|^è§£é‡Š$|è§£é‡Šä¸€ä¸‹|è§£é‡Šä¸‹|explain|why|ä¸ºä»€ä¹ˆ)", q_low):
        return {"next_node": "grad_plan", "explain_only": True, "route_pref": state.get("route_pref","auto")}

    # æ˜¾å¼è·¯çº¿åå¥½
    route_pref = "auto"
    # æ”¯æŒè¯¾ç¨‹å·/åŒä¹‰è¯è§¦å‘
    if re.search(r"(research|thesis|9991|9992|9993|ç ”ç©¶|è®ºæ–‡)", q_low):
        route_pref = "research"
    if re.search(r"(project|capstone|9900|gsoe9010|gsoe9011|é¡¹ç›®|æ¯•è®¾|è¯¾è®¾)", q_low):
        route_pref = "project"
    # è§£ææ’é™¤åå¥½ï¼ˆè¯¾ç¨‹å·/ä¸»é¢˜ï¼‰
    ex_codes, ex_topics = parse_exclusions(q)

    # å­¦ä½è§„åˆ’/é€‰è¯¾å»ºè®®
    if re.search(r"æ¯•ä¸š|æ ‡å‡†|å­¦ä½|æ–¹æ¡ˆ|è·¯å¾„|è§„åˆ’|å­¦ä¹ è®¡åˆ’|ä¸¤å¹´|é€‰è¯¾å»ºè®®|study\s*plan|degree\s*plan|roadmap", q_low):
        return {"detail_type": "", "route_pref": route_pref, "exclude_codes": ex_codes, "exclude_topics": ex_topics, "next_node": "grad_plan"}

    detail_type = ""
    if re.search(r"å‰ç½®|å…ˆä¿®|prereq|pre-?requisite|requirement|enrolment", q_low):
        detail_type = "prereq"
    elif re.search(r"äº’æ–¥|æ’æ–¥|exclusion", q_low):
        detail_type = "exclusion"
    elif re.search(r"ç­‰ä»·|ç›¸å½“|equivalent", q_low):
        detail_type = "equivalent"
    elif re.search(r"ç±»å‹|ç±»åˆ«|category|å±äºä»€ä¹ˆç±»å‹", q_low):
        detail_type = "category"
    elif re.search(r"æè¿°|ç®€ä»‹|ä»‹ç»|å†…å®¹|å¤§çº²|syllabus|description|overview|about|æ˜¯ä»€ä¹ˆ|what\s+is|è®²ä»€ä¹ˆ|å­¦ä»€ä¹ˆ", q_low):
        detail_type = "desc"

    if detail_type:
        return {"detail_type": detail_type, "route_pref": route_pref, "next_node": "detail"}
    else:
        return {"route_pref": route_pref, "next_node": "detect"}

# =========================================================
# Graph
# =========================================================
class CourseState(TypedDict):
    explain_only: bool
    query: str
    intent: str
    course_code: str
    topic: str
    num_courses: Any
    detail_type: str
    route_pref: str
    next_node: str
    answer: Annotated[List, add_messages]

graph = StateGraph(CourseState)
graph.add_node("router", router_node)
graph.add_node("detect", detect_intent_node)
graph.add_node("chitchat", chitchat_node)
graph.add_node("search", search_course_node)
graph.add_node("term_query", term_query_node)
graph.add_node("detail", detail_query_node)
graph.add_node("recommend", recommend_course_node)
graph.add_node("plan", plan_course_node)
graph.add_node("grad_plan", grad_plan_node)
graph.add_node("export", export_node)
graph.add_node("export_ics", export_ics_node)
graph.add_node("reviews", reviews_node)

graph.add_conditional_edges(
    "router",
    lambda state: state["next_node"],
    {"detail": "detail", "grad_plan": "grad_plan", "reviews": "reviews", "export": "export", "export_ics": "export_ics", "detect": "detect"},
)
graph.add_conditional_edges(
    "detect",
    lambda state: state["next_node"],
    {"chitchat": "chitchat", "search": "search", "term_query": "term_query",
     "recommend": "recommend", "plan": "plan", "grad_plan": "grad_plan"},
)
for node in ["chitchat","search","term_query","detail","recommend","plan","grad_plan","reviews","export","export_ics"]:
    graph.add_edge(node, END)
graph.set_entry_point("router")
app = graph.compile()

# === Web å‰ç«¯å…¥å£ï¼šå•è½®åº”ç­” ===
def agent_respond(user_text: str) -> str:
    """
    è¾“å…¥ä¸€æ®µç”¨æˆ·æ–‡æœ¬ï¼Œè¿”å›æœ¬è½®ç­”æ¡ˆå­—ç¬¦ä¸²ã€‚
    - å¤ç”¨å·²ç¼–è¯‘çš„ graphï¼ˆappï¼‰ä¸å…¨å±€ç¼“å­˜ï¼ˆLAST_PLAN_*ï¼‰ã€‚
    """
    try:
        compiled = globals().get("app", None)
        if compiled is None:
            compiled = graph.compile()
            globals()["app"] = compiled
        out = compiled.invoke({"query": str(user_text)})
        if isinstance(out, dict) and "answer" in out:
            # å°†æ¶ˆæ¯æ•°ç»„æ‹¼æ¥æˆæ–‡æœ¬
            msgs = out["answer"]
            parts = []
            for a in msgs:
                try:
                    parts.append(a.content if hasattr(a, "content") else str(a))
                except Exception:
                    parts.append(str(a))
            return "\n".join(parts).strip()
        return str(out)
    except Exception as e:
        return f"âš ï¸ å‡ºé”™äº†ï¼š{e}"


# =========================================================
# Main loop
# =========================================================
if __name__ == "__main__":
    print("\nğŸ“ UNSW Course Advisor Agent å¯åŠ¨æˆåŠŸï¼")
    #print("ğŸ“„ ä½¿ç”¨æ•°æ®:", CSV_FILE)
    print("ğŸ’¬ hi ä½ å¯ä»¥é—®æˆ‘å…³äºé€‰è¯¾çš„å„ç±»é—®é¢˜å“¦~ \n")
    print("é—®å®Œé—®é¢˜ è¾“å…¥exité€€å‡ºå“¦~ \n")

    context_memory = ""
    while True:
        q = input("ğŸ’¬ è¯·è¾“å…¥é—®é¢˜ï¼š")
        if q.lower() in ["exit","quit","q"]:
            print("ğŸ‘‹ å†è§ï¼"); break
        try:
            full_query = context_memory + f"\nç”¨æˆ·ï¼š{q}"
            result = app.invoke({"query": full_query})
            all_answers = result.get("answer", [])
            output_lines = [a.content if hasattr(a, "content") else str(a) for a in all_answers]
            print("\nğŸ¤– å›å¤ï¼š\n" + "\n".join(output_lines) + "\n")
            context_memory += f"\nç”¨æˆ·ï¼š{q}\nåŠ©æ‰‹ï¼š{output_lines[-1] if output_lines else ''}"
            context_memory = "\n".join(context_memory.splitlines()[-8:])
        except Exception as e:
            print(f"âš ï¸ å‡ºç°é”™è¯¯ï¼š{e}\n")
